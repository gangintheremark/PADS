{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6becf2f1-cf7b-49d6-b364-60c50302383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/j-k11s103/notebooks/pads')\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff45fb5-dc25-46c8-b157-30b7bf257917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PadsDataset import PadsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12a5147-a849-44ea-ba59-6ee5973c7d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from coca_pytorch.coca_pytorch import CoCa\n",
    "from vit_pytorch.simple_vit_with_patch_dropout import SimpleViT\n",
    "from vit_pytorch.extractor import Extractor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf31ec1-334e-4b8d-b908-447005ddfc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Extractor(\n",
       "  (vit): SimpleViT(\n",
       "    (to_patch_embedding): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1=32, p2=32)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "      (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (patch_dropout): PatchDropout()\n",
       "    (transformer): Transformer(\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (to_latent): Identity()\n",
       "    (linear_head): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit = SimpleViT(\n",
    "    image_size=224,     # Input image size\n",
    "    patch_size=32,      # Patch size\n",
    "    num_classes=1,\n",
    "    dim=1024,           # Model dimension\n",
    "    depth=6,            # Number of transformer blocks\n",
    "    heads=16,           # Attention heads\n",
    "    mlp_dim=2048,       # MLP dimension in each transformer block\n",
    "    patch_dropout=0.5   # Patch dropout rate\n",
    ")\n",
    "\n",
    "# Use Extractor to get embeddings only\n",
    "vit = Extractor(vit, return_embeddings_only=True, detach=False)\n",
    "vit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513c15bc-6df1-4bc8-9691-b462f4abba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "coca = CoCa(\n",
    "    dim=512,\n",
    "    img_encoder=vit,\n",
    "    image_dim=1024,\n",
    "    num_tokens=32000,           # Text vocabulary size\n",
    "    unimodal_depth=6,\n",
    "    multimodal_depth=6,\n",
    "    dim_head=64,\n",
    "    heads=8,\n",
    "    caption_loss_weight=1.0,\n",
    "    contrastive_loss_weight=1.0\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a06a12f-ae6e-4023-a061-9d11f60066c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camiller/Korean_Llama_Tokenizer\", use_fast=False)\n",
    "dataset = PadsDataset(img_dir=\"dataset/images\", text_dir=\"dataset/text\", tokenizer=tokenizer, transform=transform, data_type=\"Both\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c94d33-0804-433f-aecc-36165f1effb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5735b39-7b45-4d82-af57-79602a154c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coca_model(coca, dataloader, epochs=1, lr=1e-4):\n",
    "    optimizer = torch.optim.Adam(coca.parameters(), lr=lr)\n",
    "    coca.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # tqdm progress bar for each epoch\n",
    "        with tqdm(dataloader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "            \n",
    "            for batch in tepoch:\n",
    "                images = batch[\"image\"].to(device)\n",
    "                text_tokens = batch[\"text\"].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss = coca(text=text_tokens, images=images, return_loss=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Update tqdm progress bar\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Avg Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc42395a-c09f-416a-bdef-e7d2dbb06f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 계산 함수\n",
    "def calculate_cosine_similarity(text_embeds, image_embeds):\n",
    "    text_embeds = F.normalize(text_embeds, p=2, dim=1)\n",
    "    image_embeds = F.normalize(image_embeds, p=2, dim=1)\n",
    "    cosine_similarity = torch.mm(text_embeds, image_embeds.T)\n",
    "    return cosine_similarity\n",
    "\n",
    "# 훈련 후 샘플 데이터에 대한 유사도 계산 및 결과 출력 함수\n",
    "def display_sample_similarity(coca, dataloader, sample_count=5):\n",
    "    coca.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= sample_count:\n",
    "                break\n",
    "            \n",
    "            # 텍스트와 이미지 준비\n",
    "            images = batch[\"image\"].to(device)\n",
    "            text_tokens = batch[\"text\"].to(device)\n",
    "            text_data = batch[\"text\"]  # 원본 텍스트 데이터\n",
    "            \n",
    "            # 텍스트 및 이미지 임베딩 추출\n",
    "            text_embeds, image_embeds = coca(text=text_tokens, images=images, return_embeddings=True)\n",
    "            cosine_similarity = calculate_cosine_similarity(text_embeds, image_embeds)\n",
    "            \n",
    "            # 결과 출력\n",
    "            print(f\"Sample {i+1}\")\n",
    "            \n",
    "            # 텍스트 출력\n",
    "            print(\"Text:\")\n",
    "            print(text_data)\n",
    "\n",
    "            # 이미지 출력\n",
    "            plt.imshow(images[0].cpu().permute(1, 2, 0))  # 배치 첫 이미지 출력\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            # 유사도 출력\n",
    "            print(\"Cosine Similarity:\")\n",
    "            print(cosine_similarity[0])  # 첫 번째 텍스트 임베딩에 대한 유사도\n",
    "            \n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959f0d7-9679-43e6-aef6-a665ecce3651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]:  46%|████▌     | 2296/4975 [31:06<34:14,  1.30batch/s, loss=3.68] "
     ]
    }
   ],
   "source": [
    "# Train the CoCa model\n",
    "train_coca_model(coca, dataloader, epochs=5, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfcce6-3656-41e8-9514-d9f24f472f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample_similarity(coca, dataloader, sample_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ae958-6097-49b0-b198-d9cf032c60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model_save_path = \"/models/coca_model_test.pth\"\n",
    "torch.save(coca.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c192e-09ff-4585-9ed4-24711706988a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s103",
   "language": "python",
   "name": "s103"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
